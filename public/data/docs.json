[
  "Name: Aswin Vishwaa\nRole: Full-Stack & AI Developer\nGitHub: https://github.com/aswin-vishwaa\nEmail: aswinvishwaa@gmail.com\nI specialize in building intelligent applications using LLMs, computer vision, and modern web stacks. Experienced in building RAG-powered assistants, AI tools, and real-time visual systems. Passionate about deploying fast, free-tier solutions using tools like Groq, Twilio, and transformers.",
  "About Me: Iâ€™m Aswin Vishwaa â€” a Full-Stack and AI Developer who didnâ€™t start out aiming for AI or big dreams. I just wanted a job. I began with C, Python, and Java, solving basic problems without even knowing what they were building toward. My real journey started when I entered an AI course in college â€” not because I was aiming for it, but because it landed on my path. Before any code, I spent my first year as an active NCC Air Wing cadet, attending camps, talking with strangers, leading groups. That year didnâ€™t make me a coder â€” it made me a communicator, a leader, and someone who can stand on a stage without fear.\n\nMy curiosity drives me. I love learning new things just to know how they work. That mindset led me to build my first real project â€” a movie recommendation system from a YouTube tutorial. That one project flipped a switch. I dove straight into hands-on learning, skipping the textbooks and diving into tools. But my real evolution happened during the Glaucoma Screening App project â€” an AI-based diagnostic tool built for the Smart India Hackathon. Thatâ€™s where I learned the true grind: managing GPU memory, handling batch sizes, understanding the cost of bad data, and wrestling with model deployment. That project made me a developer â€” not just a student.\n\nI'm obsessed with building things that work â€” fast. Not because itâ€™s a flex, but because Iâ€™m a student. I donâ€™t have money to spend on fancy servers or paid APIs. I push the free tier to its absolute edge. I use Groq, Twilio, and every open tool I can find â€” because I have to, and because it's a challenge I enjoy. My style? Speed over polish, proof-of-concept over perfection. If it works, it ships. I'll clean it later.\n\nI work solo â€” not by choice, but by nature. I havenâ€™t done big team projects yet, but I adapt fast. My workflow is brutally direct: read the problem, research the end-to-end plan, build in sections, test as I go. I don't write clean code at first. I write working code. Once I see itâ€™s possible, I optimize. Chaos first, clarity later.\n\nOne recent example? I got two days to build a full-stack AI intern project. Never touched Docker, LangChain, or PostgreSQL before. I had no time to install PostgreSQL locally, so I threw it in Docker â€” all I knew was \"Docker is like a tiffin box for your app.\" That was enough. I used what I learned on the fly, brainstormed walking around alone, and built something functional from scratch.\n\nWhy do I keep building? Curiosity. Hunger. I donâ€™t want my brain to rust. I make projects to make my life easier, automate the boring stuff, and prove that yes â€” it can be done. If something takes 30 minutes a day, Iâ€™ll spend a weekend building a tool to save that time forever. Sometimes my laziness becomes innovation.\n\nIâ€™m not just learning AI â€” Iâ€™m making it work, even when the odds and time arenâ€™t on my side.",
  "Project: Glaucoma Screening Web Application\n\nG-Vision is an AI-powered web application built to screen for early signs of glaucoma using fundus images of the eyes. It was developed as part of a college team submission for the Smart India Hackathon (SIH), where I personally led and built the technical backbone â€” from dataset preparation to model training and web integration.\n\nThe project addresses a real-world crisis: in India, over 90% of glaucoma cases remain undiagnosed due to a lack of affordable, accessible screening tools. G-Vision provides a portable, AI-driven solution capable of performing early detection without needing a medical specialist.\n\nThe core of the system is a custom-built U-Net model designed for medical image segmentation. After struggling to integrate initial object detection models like YOLOv9 and Faster R-CNN, I pivoted toward U-Net based on insights from research papers. I trained it from scratch using annotated fundus image datasets (e.g., Drishti-GS, REFUGE), and tuned it specifically to extract the optic cup and disc. The vertical cup-to-disc ratio (VcDR), a key biomarker for glaucoma, is calculated from these segmented regions.\n\nKey Features:\n- Dual image upload (left and right eyes)\n- U-Net based segmentation of optic cup & disc\n- VcDR calculation & diagnostic outcome (Negative/Positive)\n- Ground-truth validated results\n- Responsive web interface with original vs mask visualization\n- Slider-based comparison modal for precise image overlay\n\nChallenges included data quality, medical annotation alignment, and integrating a Python-based ML pipeline into a clean Flask web app. We also had to optimize preprocessing (resizing, normalization) and manage memory constraints during model training.\n\nAlthough the hardware device is still under development, the software is fully functional and tested. Itâ€™s not deployed publicly yet and remains in a private repository. If given more time or resources, I would retrain the model for even better clinical accuracy and build out a cloud inference pipeline.\n\nThis project changed how I think about AI. It wasnâ€™t just a model â€” it was a product aimed at saving vision. The lessons I learned here in deployment, model selection, and domain-specific ML made this my turning point as an engineer.",
  "Project: Smart Email Assistant â€“ LLM-Based WhatsApp Email Notifier\n\nThis project was born from two things: my curiosity to explore large language models (LLMs), and my personal frustration with constantly checking emails. I wanted to build something that could fetch my important emails, summarize them using AI, and deliver those summaries straight to my WhatsApp â€” where I actually pay attention. Thatâ€™s how the Smart Email Assistant was born.\n\nThe system integrates Gmail, Groq's LLaMA 3, and Twilio to form a fully functional assistant that can:\n- Automatically fetch recent emails from Gmail\n- Use LLMs to filter and summarize important messages\n- Deliver summaries to WhatsApp via Twilio\n- Let the user reply with simple commands like '1' or '1R' to read summaries or generate professional responses\n\nThis wasnâ€™t just an experiment â€” it became part of my workflow.\n\nğŸ› ï¸ Technology Stack:\n- Groq API (LLaMA 3): Summarizes emails and generates tone-matching replies\n- Gmail API: Fetches recent unread emails using OAuth 2.0\n- Twilio API: Sends WhatsApp messages and receives replies\n- Flask: Backend server that handles webhook events\n- ngrok: Tunnels localhost to the public web for Twilio integration\n- JSON context store: Maintains user state and email index\n\nğŸ’¡ How it works:\n1. `gmail_fetcher.py` connects to Gmail and fetches the latest emails.\n2. Each email is passed through `llm_processor.py`, which sends it to Groqâ€™s LLM for summarization.\n3. `whatsapp_bot.py` formats and sends a clean WhatsApp message with numbered emails.\n4. If the user replies with `1`, `2`, etc., the webhook in `webhook_handler.py` serves the right summary.\n5. If they reply `1R`, `2R`, etc., the LLM generates a professional reply which is sent back instantly.\n\nâœ… Reply Commands:\n- `1`: View summary of Email #1\n- `1R`: Generate and send a professional reply for Email #1\n\nğŸ“ Folder Architecture Highlights:\n- `main.py`: Entry point for fetching & sending\n- `webhook_handler.py`: Listens to WhatsApp replies\n- `llm_processor.py`: Handles summarization & reply generation\n- `context_store.py`: Manages context via JSON\n\nğŸ” The `.env` file securely stores all API keys and credentials, including Groq, Gmail, and Twilio auth tokens.\n\nThis project taught me how to integrate LLMs into real-world automation tasks, how to maintain conversational state over a stateless API like WhatsApp, and how to build truly useful tools for daily productivity. It also showed me how powerful LLMs can be when paired with thoughtful backend workflows. What started as a lazy solution became one of my most practically useful AI tools to date.",
  "Project: Doctor-Agent â€“ LLM-Powered Appointment and Reporting System\n\nDoctor-Agent is a production-ready, full-stack AI assistant that allows patients to book doctor appointments through natural language, and enables doctors to retrieve appointment and symptom-based analytics in real time. This solo project was built as part of an interview assignment during my internship selection process, giving me only a couple of days to ideate, research, and deliver a functional end-to-end system.\n\nThe assistant combines LangChain, Groq LLaMA 3, FastAPI, React, Docker, PostgreSQL, and Google Calendar into a cohesive agentic system using the MCP (Model Context Protocol) architecture. Patients can book appointments with free-text prompts like 'I need to see Dr. Mehta tomorrow at 10 AM', while doctors can ask things like 'How many patients had fever this week?' and receive structured responses.\n\nThis project is a prime example of how I dive straight into hands-on problem-solving before touching theory. I had zero prior experience with Docker, PostgreSQL, or LangChain, but I integrated all three under pressure by learning on the fly. To save setup time, I used Docker as a containerized environment â€” treating it like a â€œtiffin boxâ€ that held the database and backend services neatly.\n\nğŸ§  Key Features:\n- Natural language appointment booking\n- Doctor analytics (appointments, symptom-based queries)\n- Google Calendar event integration\n- Email confirmation system via SMTP\n- MCP-based LLM tool registration and invocation\n- Dockerized backend, DB seeding, and modular architecture\n- Session memory preservation with LangChain's ConversationBufferMemory\n\nğŸ› ï¸ Stack Overview:\n- Frontend: React + Vite + Tailwind CSS\n- Backend: FastAPI + Docker + LangChain\n- LLM: Groq API (LLaMA 3)\n- DB: PostgreSQL (with Docker-based seeding)\n- External: Gmail SMTP, Google Calendar API\n\nğŸ—‚ï¸ Folder Structure:\n- `/backend/`: FastAPI + MCP tool registry\n- `/frontend/`: React chat UI\n- `docker-compose.yml`: Orchestrates backend, DB, and seeding\n\nğŸ“ˆ Agent Flow:\n1. Patient/Doctor enters natural prompt\n2. LLM selects the right MCP tool from registry\n3. Tool performs action (e.g., DB query, appointment booking)\n4. Result returned as human-readable response\n\nThis project proved I can break into new technologies under pressure and deliver robust, scalable results. It reflects my engineering style: speed-first execution, curiosity-driven learning, and functional integration across the full stack. Whether it's backend orchestration, agent memory handling, or working with real-time APIs â€” Doctor-Agent was built to work like a production tool, not a demo.",
  "Project: Movie Recommendation System\n\nThis was my very first project in machine learning, built as a foundation to understand how recommender systems work. It includes two separate models: one using a static dataset, and another that fetches real-time data from an external API. Although it has no frontend or UI, this project laid the groundwork for how I understand similarity metrics, content-based filtering, and API integration.\n\nğŸ§  Model 1: Static Dataset Recommender\n- Uses a local Excel (.xlsx) file as the data source\n- Recommends movies based on genres and ratings using cosine similarity\n- Basic pandas and scikit-learn implementation\n\nğŸŒ Model 2: API-Based Recommender\n- Fetches live movie data from an external API (like TMDB)\n- Builds a real-time similarity matrix for recommendations\n- Introduced me to API request handling, parsing responses, and fallback logic\n\nThis project may look simple on the surface, but it helped me grasp how raw data can be processed and converted into intelligent outputs â€” something I now apply in more advanced systems. It was also my first step toward building AI tools that interact with real-world data.",
  "Project: Developer Portfolio with RAG-Powered AI Chatbot\n\nThis is my personal developer portfolio, designed to be more than just a static webpage. It's an intelligent, interactive representation of my work â€” featuring a custom-built Retrieval-Augmented Generation (RAG) chatbot that can answer questions about my experience, projects, and skills.\n\nğŸ› ï¸ Tech Stack:\n- Frontend: React.js (Vite)\n- Styling: Tailwind CSS\n- UI: ShadCN UI, Lucide Icons\n- Animations: Framer Motion\n- Routing: React Router\n- Hosting: GitHub Pages / Vercel\n\nğŸ¤– AI Chatbot Architecture:\n- Embeddings: Local generation using @xenova/transformers (MiniLM)\n- Retrieval: Cosine similarity on a JSON-based context file (docs.json)\n- Generation: Groq API (LLaMA 3)\n- Zero backend: Entire chatbot works client-side on the browser\n\nğŸ“¦ Portfolio Sections:\n- Animated landing page with CTA buttons (Projects, Resume, Contact)\n- About section with bio, skills, and language list\n- Projects section with detailed cards, GitHub links, and filters (AI/Web)\n- Resume download & contact section\n- Embedded chatbot trained on personal data, works offline with local embeddings\n\nğŸ“ˆ Performance & Privacy:\n- Vite for fast builds and lazy loading\n- Tailwind CSS for responsive design\n- No trackers or analytics unless added manually\n- No backend: Embeddings are computed client-side\n\nThis project reflects my philosophy: show, donâ€™t just tell. Rather than listing my skills, I built a system where people can ask about my work directly â€” and the chatbot responds intelligently using real context. Itâ€™s not just a portfolio; itâ€™s a demonstration of what I build, how I think, and how I scale.",
  "Project: Bookstore Inventory App\n\nThis project was built as part of my self-learning journey through a FreeCodeCamp crash course to master the MERN stack. Itâ€™s a basic CRUD application that allows users to manage a list of books via a clean interface.\n\nğŸ§± Stack:\n- Frontend: React.js\n- Backend: Node.js + Express\n- Database: MongoDB\n\nğŸ“Œ Features:\n- Add, update, delete, and view books\n- Responsive frontend\n- RESTful API endpoints for all CRUD operations\n\nAlthough this was a basic application, it helped solidify my understanding of how frontend, backend, and databases connect. It was also the first time I set up MongoDB for real-time operations.",
  "Project: Web Scraping Automation Tool\n\nThis is a simple proof-of-concept tool built using Python and BeautifulSoup to automate data extraction from websites. It was created purely out of curiosity to see whether scraping logic could be implemented and extended.\n\nğŸ› ï¸ Tech:\n- Python\n- BeautifulSoup\n\nğŸ§  Purpose:\n- Experiment with real-world data collection\n- Build skeleton-level code to validate scraping flows\n\nThough itâ€™s not production-level or styled, the tool works as expected and marks one of my early steps toward automation and backend scripting.",
  "Project: PySpark Housing Price Prediction\n\nThis academic project focused on using PySpark for housing market analysis and price prediction. It demonstrates the use of big data pipelines, machine learning models, and real-world deployment.\n\nğŸ§  ML Workflow:\n- Preprocessing: Encoding categorical values, scaling numerical ones\n- Feature Engineering: Added derived metrics like per-sqft pricing\n- Models: Linear Regression, Decision Trees\n- Evaluation: RMSE & MAE metrics\n\nğŸš€ Deployment:\n- Flask web app hosted on Render\n- Predicts housing prices with an intuitive UI\n- Hosted at: https://hosuing-market-price-analysising.onrender.com\n\nğŸ”§ Tech Stack:\n- PySpark, Flask, Scikit-learn, NumPy, Pandas\n- Render (for deployment)\n\nThis project introduced me to large-scale ML training and integration into web apps.",
  "Project: Sign Language Recognition (Freelance)\n\nThis freelance project was completed for a senior in my college, and I earned my first â‚¹2000 from it. It recognizes alphanumeric hand gestures using a webcam feed and a CNN-powered deep learning model, modified from an open-source repository.\n\nğŸ” Features:\n- Real-time detection of 36 hand signs\n- Built using MediaPipe and TensorFlow\n\nğŸ’¼ Tech:\n- Python, OpenCV, MediaPipe\n- Keras / TensorFlow (for CNN)\n\nThe challenge was customizing the repo to match our use case. It taught me about gesture-based UI, computer vision basics, and how to deliver functional freelance projects.",
  "Project: Anime Series Front-End Page\n\nThis was my very first project â€” a static webpage built using HTML, CSS, and Bootstrap to showcase an anime series. It helped me understand how websites are structured and how VSCode, folders, and styling all work together.\n\nğŸ”§ Tech:\n- HTML\n- CSS\n- Bootstrap\n\nThe site includes:\n- Anime synopsis\n- Links to watch episodes\n- Merchandise and Q&A sections\n\nğŸŒ Live Preview: https://aswinvishwaa.github.io/project-1/\n\nIt may be simple, but it marks the starting point of my development journey.",
  "Project: Rhyme Calendar Flow\n\nThis project was built under time pressure for my girlfriendâ€™s placement training submission. She had less than 30 minutes to host a project, so I stepped in. Inspired by the idea of 'Lovable AI', I used an AI prompt to generate the base project and hosted it instantly.\n\nğŸ“Œ Concept:\n- AI-generated calendar assistant that generates poetic or rhythmic suggestions based on your schedule\n- Built using React and deployed on Vercel\n\nğŸ”§ Tech:\n- React\n- Vercel\n\nğŸŒ Live: https://react-rhythm-calendar-flow.vercel.app\n\nEven though the project is simple, it reflects how I can ideate and ship products fast â€” even under emotional and deadline pressure.",
  "Skills & Tools:\n\nğŸ§  Programming Languages:\n- Python: My primary language across all AI, ML, and automation projects (Smart Email Assistant, Sign Language Recognition, Glaucoma AI)\n- JavaScript: Used in all React projects, portfolio site, and MERN stack projects\n\nğŸ–¥ï¸ Web Development:\n- React.js: Used for frontend in Doctor-Agent, Portfolio Site, and Rhyme Calendar\n- Flask & FastAPI: Backend frameworks for projects like Email Assistant (Flask) and Doctor-Agent (FastAPI)\n- MongoDB & PostgreSQL: NoSQL (MongoDB) used in Bookstore App; SQL (PostgreSQL) used in Doctor-Agent with Docker setup\n- HTML/CSS, Bootstrap, Tailwind: Styling stack used in early projects and portfolio design\n\nğŸ§  AI/ML Tools:\n- PyTorch: Used in CNN-based Sign Language model\n- scikit-learn, Pandas, NumPy: Used for housing price prediction and recommender systems\n- HuggingFace Transformers: Used via Xenova for local embedding in portfolio RAG chatbot\n- LangChain: Integrated for agent-based behavior in Email Assistant and Doctor-Agent\n\nğŸ› ï¸ DevOps & APIs:\n- Docker: Used in Doctor-Agent for containerized deployment\n- Twilio API: Used in Smart Email Assistant to send WhatsApp alerts\n- Gmail API: Email fetching in Smart Email Assistant\n- Groq API: LLM backend for most AI projects (LLaMA 3 based)\n- Git/GitHub: Used across all projects for version control\n\nMy skillset leans heavily toward practical implementation â€” I learn tools by building, not just reading.",
  "Soft Skills & Work Style:\n\nğŸ§  Self-Learner & Rapid Prototyper:\n- I learn by doing â€” jumping into real-world projects before reading theory.\n- Built multiple solo projects by reverse-engineering, self-research, and overnight building.\n\nğŸ’¬ Communication & Public Speaking:\n- Developed strong communication skills through 1 year in NCC (Air Wing), attending national-level camps and leading teams.\n- Comfortable presenting ideas in public, mentoring peers, and documenting project workflows clearly.\n\nğŸ¤ Leadership & Independence:\n- Acted as sole developer in most of my major projects (Doctor-Agent, Glaucoma App, Smart Email Assistant).\n- Handle problem-solving under time pressure (e.g., finishing AI-based projects within 2â€“3 days).\n\nğŸ¯ Problem Solving & Execution:\n- Prefer Proof of Concept > Perfection: I aim to validate the working system first, polish later.\n- Known to use unconventional shortcuts (e.g., using Docker just to avoid native DB setup delay).\n\nâš™ï¸ Team Adaptability:\n- Though I usually work solo, I played the technical backbone role in the Glaucoma project team.\n- I can adopt to team structures quickly when necessary but thrive in autonomy.\n\nğŸ“Œ Work Style:\n- No fixed structure â€” I plan, research deeply, then build the project in sectional iterations.\n- I revise messy initial codebases into optimized solutions post-PoC.\n- Deadline-driven. If I get 2 days, Iâ€™ll deliver in 2. I sacrifice sleep before missing delivery.",
  "Languages:\n\nğŸ—£ï¸ Tamil (Native):\n- Mother tongue. Comfortable with reading, writing, and speaking fluently.\n- Used in everyday communication, technical explanations, and casual conversations.\n\nğŸ—£ï¸ English (Fluent):\n- Primary medium for coding, documentation, GitHub projects, and technical presentations.\n- Comfortable with both written and spoken English â€” used in project reports, resumes, emails, and public speaking (especially during college events and hackathons).",
  "Awards & Recognition:\n\nğŸ¥‡ 1st Prize - Startup Mania (Glaucoma Screening AI)\n- Recognized for building a practical AI-powered diagnostic tool for early glaucoma detection using U-Net segmentation.\n- Judges appreciated the medical impact, accessibility focus, and working web demo.\n\nğŸ… Top 10 Finalist - Titan Challenge (Pitch Presentation for G-Vision)\n- Reached the finals by presenting the G-Vision concept: a portable, AI-powered glaucoma screening solution using smartphone fundus cameras.\n- My role involved both technical development and communicating the medical and social value of the project.\n\nBoth awards were for the same project in different stages â€” one focused on product building, the other on pitching and impact.",
  "Blog: Portfolio RAG Chatbot Architecture\n\nğŸ” Overview:\nI built a portfolio-integrated AI chatbot using Retrieval-Augmented Generation (RAG). The goal was to create a fully client-side assistant that answers queries about my work, skills, and projects â€” without needing a backend.\n\nğŸ§  Architecture:\n- **Embeddings**: Uses `@xenova/transformers` with the `all-MiniLM-L6-v2` model for in-browser text embedding.\n- **Vector Search**: Queries are converted to vectors and compared against pre-embedded documents using cosine similarity.\n- **Top-k Retrieval**: Top 3 most relevant chunks (from resume, project docs, etc.) are selected.\n- **LLM Response**: These chunks are passed to Groq's hosted LLaMA 3 API for generating intelligent answers.\n\nğŸ’¡ Key Features:\n- Fully static â€” runs on Vercel with no backend\n- Free-tier optimized: all embedding happens locally, Groq is used only for final response\n- Secure & privacy-friendly: no data leaves the browser unless prompted\n- Smart enough to answer project-specific questions like \"Where did you use Docker?\" or \"Whatâ€™s the tech stack in Glaucoma AI?\"\n\nğŸ” Limitations:\n- Stateless (doesnâ€™t maintain chat history)\n- Not designed for long conversations â€” only for direct Q&A\n- Only reflects static, pre-indexed content\n\nğŸš€ Deployment:\n- Hosted on Vercel\n- Uses `docs.json` as the single source of knowledge (this JSON youâ€™re building now)\n- No server, database, or backend logic â€” fast, simple, and open to expansion\n\nğŸ“ Why This Blog Exists:\nTo prove that I not only *built* a RAG chatbot, but understood it deeply â€” from vector embeddings to frontend integration to LLM orchestration.\nThis project reflects my practical AI knowledge, obsession with optimization, and ability to build smart tools under free-tier constraints."
]